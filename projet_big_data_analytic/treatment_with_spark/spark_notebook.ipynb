{
  "metadata": {
    "name": "Note converted from Jupyter_2HVU4G755",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndata \u003d spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/user/hadoop/dakar_weather.csv\")\ndata"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n  data.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndata.show(10)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n%pyspark\n\nfrom pyspark.sql.functions import col, sum\n\ndata.agg(*[sum(col(c).isNull().cast(\"int\")).alias(c) for c in data.columns]).show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndata.describe().show()\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\nnum_rows \u003d data.count()\nnum_cols \u003d len(data.columns)\nprint(\"Shape of data: ({}, {})\".format(num_rows, num_cols))\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\nfrom pyspark.sql.functions import split\n\n\ndata \u003d data.withColumn(\"year\", split(data[\"time\"], \"-\").getItem(0))\ndata \u003d data.withColumn(\"month\", split(data[\"time\"], \"-\").getItem(1))\ndata \u003d data.withColumn(\"day\", split(data[\"time\"], \"-\").getItem(2))\n\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\ndata.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\r\n%pyspark\r\n\r\nfrom pyspark.sql.functions import split\r\n\r\ndata \u003d data.withColumn(\"day\", split(data[\"day\"], \"T\").getItem(0))\r\ndata \u003d data.withColumn(\"hour\", split(data[\"time\"], \"T\").getItem(1))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\ndata.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\nfrom pyspark.sql.functions import when\n\n# Suppose \u0027data\u0027 est un objet DataFrame Spark\ndata \u003d data.withColumn(\"rain\", when(data[\"rain\"] \u003e 0, 1).otherwise(data[\"rain\"]))\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\n\ndata.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\n\ndata \u003d data.drop(\"time\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\n\ndata.show()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n%pyspark\ndata.dtypes"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\r\n%pyspark\r\n\r\nfrom pyspark.sql.types import FloatType\r\n\r\ndata \u003d data.withColumn(\"temperature\", data[\"temperature\"].cast(FloatType()))\r\ndata \u003d data.withColumn(\"apparentTemperature\", data[\"apparentTemperature\"].cast(FloatType()))\r\ndata \u003d data.withColumn(\"windSpeed\", data[\"windSpeed\"].cast(FloatType()))\r\ndata \u003d data.withColumn(\"pressure\", data[\"pressure\"].cast(FloatType()))\r\n\r\n\r\nfrom pyspark.sql.types import IntegerType\r\n\r\ndata \u003d data.withColumn(\"relativeHumidity\", data[\"relativeHumidity\"].cast(IntegerType()))\r\ndata \u003d data.withColumn(\"weathercode\", data[\"weathercode\"].cast(IntegerType()))\r\ndata \u003d data.withColumn(\"rain\", data[\"rain\"].cast(IntegerType()))\r\ndata \u003d data.withColumn(\"year\", data[\"year\"].cast(IntegerType()))\r\ndata \u003d data.withColumn(\"month\", data[\"month\"].cast(IntegerType()))\r\ndata \u003d data.withColumn(\"day\", data[\"day\"].cast(IntegerType()))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n%pyspark\ndata.dtypes"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\r\n\r\nfrom pyspark.sql.functions import concat_ws\r\n\r\ndata \u003d data.withColumn(\"date\", concat_ws(\"-\", data[\"year\"], data[\"month\"], data[\"day\"]))"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\r\n\r\ndata.show()\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\ndata.dtypes"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "// %pyspark\r\n\r\n// from pyspark.sql.functions import to_date\r\n// from pyspark.sql.types import DateType\r\n\r\n// data \u003d data.withColumn(\u0027date\u0027, to_date(data[\u0027date\u0027], \u0027yyyy-MM-dd\u0027))\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\r\ndata.dtypes"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\r\n\r\nfrom pyspark.sql.functions import date_format\r\n\r\ndata \u003d data.withColumn(\"weekday\", date_format(data[\u0027date\u0027], \"EEEE\"))\r\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndata.show(184079)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "\n\n%pyspark\n\ndata.write.csv(\"dakarWeather.csv\", header\u003dTrue, mode\u003d\"overwrite\")\n"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n"
    }
  ]
}